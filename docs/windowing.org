#+FILETAGS: windowing
#+SEQ_TODO: TODO ONGOING FOLLOWUP DONE
* Modes
 | Mode     | Translator      | Executor     |
 |----------+-----------------+--------------|
 | Local    | LocalTranslator | Executor     |
 | TEST     | LocalTranslator | TestExecutor |
 | HIVE     | HiveTranslator  | Executor     |
 | HIVETEST | HiveTranslator  | TestExecutor |
 | MR       | MRTranslator    | MRExecutor   |
 | MRTEST   | MRTranslator    | MRExecutor   |
 |----------+-----------------+--------------|

- Translators differentiated by how they setup the Queries WindowingInput
  - LocalTranslator:
    - expects complete details of input: keyClass, valueClass,
      iFmtCls, serDeCls, columnNames, columnTypes and WindowingInputCls
    - works with TableWindowingInput; reads input using FileSystem API
    - was used when not tied to Hive Metastoreserver
    -ni more used
  - HiveTranslator:
    - used when Hive Script Operator spawns WindowingDriver
    - not actively maintained
  - MRTranslator:
    - works with MRWindowingInput
    - mRWindowingInput needs a tableName in the Query; and extracts
      the SerDe by talking to the MetaStoreServer
- Executors: differentiated by how result rows are output
  - TestExecutor:
    - configured with a PrintStream; default is System.out
    - output row is created as a list and printed to PrintStream
    - used in BasicTests.
  - MRExecutor:
    - is bridge between Query & mR Job.
    - Setups a HiveConf for the Job
    - sets the Query and temptable created(for hive query) in conf
    - invokes job.run 

- Why do we have MR & MRTEST modes
  - utilized by WindowingDriver to decide how to set WINDOWING_JAR_FILE
  - this needs to be cleaned up

* Default Query Execution
#+begin_src dot :file defaultQryExec.png :cmdline -Kdot -Tpng
digraph G {
  size ="6 6";
  nodesep=.2;
  //rankdir=BT;
  ranksep=.25;
  node [shape=record];
  compound=true;

  input[label="Raw Stream"]
  wi[label="QueryInput:WindowingInput"]
  input -> wi [label="bytes"];
  iserde[label="QueryInput:SerDe"];
  wi -> iserde [ label="Writable"];
  copyToStdOI[label="CopyToStandardOI" shape="ellipse"];
  iserde -> copyToStdOI [label="native Object"];
  partition[label="{Partition|<e1>elem1|...|<en>elemN}"];
  copyToStdOI -> partition:e1 [label="java Array"];
  iObj[label="Input Object"];
  iObj -> partition:en [label="wraps"];
  ge[label="{Execution env.\n(Groovy based)|{{WFn1|..|WFnN}|{OExpr1|..|OExprN}}}"];
  iObj -> ge;
  resultMap[label="{ResultMap|name = [..]|name = [..]|...}"];
  ge -> resultMap [label="output of\nevaluation of a Partition"];
  oObj[label="Output Object"];
  iObj -> oObj;
  resultMap -> oObj;
  oObj -> ge [label="eval. output Exprs"];
  olist[label="Output Row as list"];
  ge -> olist;
  oserde[label="QueryOutput:Serializer"];
  olist -> oserde;
  ordrwriter[label="QueryOutput:RecordWriter"];
  oserde -> ordrwriter[label="Writable"];
  output[label="Raw Stream"];
  ordrwriter -> output [label= "bytes"];

  {rank=same; ge; olist};
  //{rank=same; iserde; iObj};
}
#+end_src

#+results:
[[file:defaultQryExec.png]]

- The execution of Windowing Functions & expressions happen in a
  GroovyShell. There is a GroovyShell associated with each
  RunningContext. This Shell is attached to the Query.
** The Input Row
- Each Query has an associated WindowingInput. This is used to read
  the next structured row.
- This Object is converted to a ArrayList using ObjectInspectorUtils.copyToStandardObject
- These are held as sets in a Partition Object.
- An InputObject is a Groovy Binding that is associated with an index
  of the Partition object.
- When a value is requested from it; it gets the value from the
  underlying StandardOI.
*** DONE deal with the original structure; avoid converting to a StandardObject
    CLOSED: [2012-01-25 Wed 20:28]
    CLOCK: [2012-01-25 Wed 15:30]--[2012-01-25 Wed 20:00] =>  4:30
    CLOCK: [2012-01-24 Tue 17:00]--[2012-01-24 Tue 18:38] =>  1:38
** Partitioner
- is responsible for breaking up a stream of Input rows into a stream of Partitions.
- it calls on the WindowingInput for the next set of rows and collects them into a Partition object 
*** DONE optimize comparison in Partition object; Convert first row of Partition only once.
    CLOSED: [2012-01-24 Tue 17:00]
    CLOCK: [2012-01-24 Tue 16:45]--[2012-01-24 Tue 17:00] =>  0:15
** The processing of Windowing Functions
- wrap the input stream in a Partioner. Now process each Partition
- Maintain a ResultMap( ExpressioName -> List )
- for each WindowingFunction invoke processPartition(p).
  - the processPartion contract requires the WFunction to return a
    list whose size is equal to the Partition's size. The elements of
    the list are of type specified by the Function's signature.
** The Output Object
- is also a Groovy Binding.
- it encapsulates an InputObject
- it also encapsulates the ResultMap for a partition.
- a name is resolved either as a evaluated value or as an input
  column. The InputObject associated with it specifies the row in the
  Partition that is used.
- Output Object also provides functions usable in the select
  clause. Currently it has the lead and lag functions.
** The generation of Output Rows for a Partition
- An OutputObj is associated with the Partition and its ResultMap
- now for each row in the Partition:
  - the input row is bound to the OutputObj
  - the whereClause is evaluated to decide if this row needs to be Output
  - if yes:
  - an output array is constructed by evaluating expression for the OutputColumns of the Query.
  - The Array is serailized using the Serializer associated with the query
  - and wriiten using the RecordWriter associated wit the Query.
* Windowing CLI operation
** Overall Architecture
#+begin_src dot :file cliOperation.png :cmdline -Kdot -Tpng
digraph G {
  size ="6 6";
  nodesep=.2;
  //rankdir=LR;
  ranksep=.25;
  node [shape=record];
  compound=true;

        subgraph cluster0 {
          //color=lightgrey;
         style=bold;
         label = "HiveCLI(windowing-extension)"; fontsize=24;
                input[shape=record, label="Console"];
                hiveCli[label="Hive Cli"];
                windowingClient[label="Windowing Client"];
                input -> hiveCli[label="hive\nMode"];
                input -> windowingClient[label="windowing\nMode"];
       };
       subgraph cluster1 {
         //color=lightgrey;
         style=bold;
         label = "Windowing Server"; fontsize=24;
                wshell[label="Windowing Shell"];
       };
       subgraph cluster2 {
         //color=lightgrey;
         style=bold;
         label = "Hive/Hadoop"; fontsize=24;
                hmetaserver[label="Hive MetaStoreServer"];
                hserver[label="Hive Server"];
                hcluster[label="{Hadoop Cluster|{{{MR|HDFS}|node1}|...|{{MR|HDFS}|nodeN}}}}"];
                hmetaserver -> hcluster;
                hserver -> hcluster;
       };

       input -> wshell[label="spawn", ltail=cluster0, rhead=cluster1];
       windowingClient -> wshell[label="executeQuery", rhead=cluster1];
       wshell -> windowingClient[label="callback\nexecute\nHQL", style="dotted"];
       hiveCli -> hmetaserver;
       hiveCli -> hcluster[label="exec MR jobs\n for HQL"];
       wshell -> hmetaserver;
       wshell -> hcluster[label="exec MR job\n for WQry"];

       {rank=same; input; wshell};
}
#+end_src

#+results:
[[file:cliOperation.png]]
** Query execution
#+begin_src plantuml :file cliQueryExecFlow.png
  title Query Execution in WindowingCLI
  autonumber
  actor User
  User->WindowingCliDriver: execute a Query
  alt wmode is hive
  WindowingCliDriver -> HiveCliDriver: processCmd
  HiveCliDriver --> User: Response
  else wmode is windowing
  participant WindowingService as WC
  box "Windowing Server" #LightBlue
     participant Listener as WS
     participant ExecutorService
     participant Connection
     participant WShell
  end box
  WindowingCliDriver -> WC: executeQuery(cmd)
  WC -\ WS: sendRequest
  WS -> ExecutorService: submitRequest
  ExecutorService ->o Connection: handleRequest
  Connection -> WShell: execute(qry)
  Activate WShell
  alt qry contains hive query
  WShell -> Connection: createTempTable
  Connection -> WC: handleQuery
  WC -> HiveCliDriver : processCmd
  HiveCliDriver --> WC : Response
  WC --\ Connection: Response
  Connection -> WShell
  end
  WShell -\ HadoopCluster: run Windowing Job
  HadoopCluster --\ WShell : job finished
  WShell --> Connection : Response(error/ok)
  DeActivate WShell
  Connection --\ WC
  WC --\ WindowingCliDriver
  WindowingCliDriver --> User
  end
#+end_src

#+results:
[[file:cliQueryExecFlow.png]]

* Reading and Writing data
- query is associated with a input Deserializer and an output SerDe.
** The WindowingInput interface
- extends the RecordReader interface. The RecordReader interface
  provides a way for rows to flow as Writables from the Hive Script Operator to an
  external program. This was chosen because the first integration
  developed was via the Script Operator.
- A WindowingInput is also responsible for providing a SerDe
  for the input. So that raw Writables can be transformed to
  structured objects.
- A WindowingInput is setup during translation by the
  setupWindowingInput(Query) call. A WindowingInput class is closely
  associated with a particular Translator.
*** LocalTranslator and TableWindowingInput
- used to read directly from a File. Bypasses hive metadata
  layer. Hence Query must specify all the details.
- the query must specify:
  - the Key & Value Classes
  - the InputFormatClass
  - the SerDe class; the Column Names and Types of records
  - the location of the directory containing the data files
- TableWindowingInput then
  - creates a SerDe instance
  - validates the tablePath
  - setups an InputFormat instance
  - reads the splits for the Path using the FileSystem API
  - sets itself for iteration of the input rows; gets the RecordReader
    from the first split
  - the iteration logic drains the current RecordReader. Once done it
    tries to open the next split if it exists and starts returning
    rows from it; otherwise it stops
*** HiveTranslator & HiveWindowingInput
- this is used when the Hive Script Operator spawns a Windowing Process
- The Script Operator is configured to stream data using the TypedBytesRecordWriter
- So on the WindowingRecordReader side the corrsponding
  TypedBytesRecordReader is used to read Writables from the raw stream.
- The Reader is configured to read from System.in, as this is the
  mechanism used by Hive Script Operator
- The Query must provide details about the structure of the
  records. So the query must contain:
  - the SerDe class; the Column Names and Types of records
*** MRTranslator & MRWindowingInput
- this is used when Windowing operations run in the Reduce Phase of a
  MR Job.
- In this situation the WindowingInput is not as a source of
  Writables; but it still used to provide the Deserializer to convert
  a raw Writable into a structured record.
- The information about the table is read by making a connection to a
  Hive MetaStore Server.
- The Query only needs to refer to a Hive Table.
**** handling embedded Hive Query
- in place of a Hive table a hive query may alternatively be specified
- in this case the Query is wrapped in a CTAS statement; the tableName
  is generated using the currentTimeMillis(); and the temptable is
  used as the input to the Query.
** Writing Result Rows
- the translator infers the types and names of the columns of the
  output record.
- currently the OutputSerDe is hard-coded: (1/13/12 this is no more the case, see [[Enhacing Output Behavior]])
  - LazySimpleSerDe in MR mode
  - TypedBytesSerDe in other modes
*** MR mode
- the MR job is configured using the outputformat specified in the
  Query. Currently tested with TextOutputFormat.
- The output SerDe is fixed to  LazySimpleSerDe
- In the Reduce function the output columns are converted using the LazySimpleSerDe
- And then passed to the OutputCollector
*** Default behavior (non MR mode)
- data is wriiten to System.out
- data is written using the TypedBytesRecordWriter
- so in the case of Hive Mode
  - output is streamed back to the Hive Script Operator as Writables
    which are read using the corresponding TypedBytesRecordReader
- in Local mode data is written to System.out using the same TypedBytesRecordWriter
- there is a TestExecutor that shortcircuits writing output by
  directly writing the output columns of a row as an array to System.out
** Enhacing Output Behavior
*** The Query Interface
| parameter           | description                         | default                |
|---------------------+-------------------------------------+------------------------|
| MR mode:            |                                     |                        |
| output\_file\_name  | where the Output should be stored   | required               |
| output\_serde,      | specify serde and fileformat class, | TypedBytesSerDe,       |
| output\_fileformat  | and serde properties                | User-specified         |
|                     |                                     |                        |
| Hive/Local mode:    |                                     |                        |
| output serde        |                                     | TypedBytesSerDe        |
| output RecordWriter |                                     | TypedBytesRecordWriter |
|                     |                                     |                        |

- Output clause
#+begin_src sql
into PATH=<user specified path>
[ SERDE <serdeClass> (WITH SERDEPROPERTIES namevalue*)?
  ( RECORDWRITER <recordwriterClass> |
    OUTPUTFORMAT <outputformatClass>
  )
]
#+end_src
- QuerySpec:TableOutput datastruct
#+begin_src java
class TableOutput
{
  String path
  String serDeClass = "org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe"
  Properties serDeProps = new Properties()
  String formatClass = 'org.apache.hadoop.mapred.TextOutputFormat'
  String recordwriterClass = "org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter"
  public String toString()
}
#+end_src  
- Query;QueryOutput
#+begin_src java
class QueryOutput
{
  ArrayList<OutputColumn> columns = []
  StructObjectInspector outputOI;
  SerDe serDe
  StructObjectInspector processingOI;
  RecordWriter wrtr
}
#+end_src

- Translation hooks:
  1. validateOutputSpec
     - MR: valid serDe, format; no writer
     - Rest: valid serDe, writer; no outputformat
  2. getOutputSerDe() : based on QSpec serDeClass
  3. setupOutputWriter()
     - not needed in MR mode; but validate formatClass is valid.

* Composite Record mechanics
** Types
- a DataType<T> captures type information about a WritableComparable
  class
- A DataType can be asked to create, cast, clone the type T it represents
- It also has a RawComparator and can be asked to raw compare to byte
  arrays representing instances.
- Basic DataTypes are BOOLEAN, BYTE, SHORT, INT, VINT, LONG, VLONG,
  FLOAT, DOUBLE and TEXT
*** CompositeDataType and CompositeWritable
- represents a structure of DataTypes
- structs may contain structs
- a CompositeDataType can be created from a Hive StructObjectInspector
- a CompositeWritable represents an instance of a CompositeDataType struct
*** CompositeSerialization
- a CompositeDeserializer casts Writable to the approriate
  CompositeType before invoking readFields.
- Conf parameter "windowing.composite.datatype" specifies the
  CompositeDataType details. A CompositeDataType instance is
  instantiated based on the valueof this parameter
* Windowing processing in a MR Job
** Job definition
*** Configure Hive Table as Job Input
- get the Hive Table details from the Hive MetaStoreServer
- add the StorageDescriptor location as the inputPath for the Job
- set the Job's InputFormat class based on the information the StorageDescriptor
- set the Job's MapOutputValueClass based on the InputFormat(get it
  from its RecordReader)
*** Type Information
- From the Query's sort & partition columns a ObjectInspector and then
  a CompositeDataType is constructed. This is added to the Job
  Conf. The order of the columns is the partition columns followed by
  the sort columns.
- Job "io.serialization" is set to [[CompositeSerialization]]
*** Remaining Parameters
| Param                                 | Value                    | Notes                            |
|---------------------------------------+--------------------------+----------------------------------|
| jar                                   | windoingJar file         | enables these jobs w/o           |
|                                       | specified in the running | having to add the jar            |
|                                       | context                  | to the task nodes beforehand.    |
| OutputValueClass                      | Text                     | hardcode for now                 |
| MapOutputKeyClass                     | [[CompositeWritable]]        | used to extract fields in        |
|                                       |                          | Partition + Sort clause.         |
| OutputKeyClass                        | NullWritable             |                                  |
| PartitionerClass                      | [[Partitioning]]             | partition only by Part. columns  |
| OutputKeyComparatorClass              | OutputKeyComparator      |                                  |
| OutputValueGroupingComparator         | [[OutputGroupingComparator]] |                                  |
| windowing.input.database              | db set in Qry            | currently always null            |
| windowing.input.table                 | table from Qry           |                                  |
| windowing.partition.cols              | from Qry                 | represented as comma seperated   |
|                                       |                          | String                           |
| windowing.sort.cols                   | from Qry                 | represented as comma sep. String |
| windowing.sort.order                  | from Qry                 | comma sep; ASC/DESC list         |
| windowing.number.of.partition.columns | computed from Part. list |                                  |
| windowing.query.string                | the Qry                  | currently Qry is reparsed at     |
|                                       |                          | each Reducer                     |
| windowing.hivequery.temptable         | name of TempTable        | the TT created for the embedded  |
|                                       |                          | Hive Query.                      |
|---------------------------------------+--------------------------+----------------------------------|

** Job Execution
*** Map Job
- On Configure read the sortCols and CompositeDataType from the Conf
- Map function: create a CompositeWritable containing the columns from
  the Partition + Sort lists. Output this CompositeWritable and the
  input Writable as the Key, Value
*** Partitioning
- we want all the rows having the same values for the Partition
  columns to go to the same reducer.
- the Partition class uses the "windowing.number.of.partition.columns"
  param to only compare based on the p first elems of the
  CompositeWritable.
*** OutputGroupingComparator
- configured to sort rows in a Reduce Partition.
- in this case these are sorted based on all the elements in the CompositeWritable.
*** Reduce Job
- operates almost identically to the [[Default Query Execution]]
  - Partitions are formed manually by creating a Partition object
  - Output Writables are written to the OutputCollector.
**** currently the query String is passed in the Job Conf. Hence each Reducer reparses the Query.
***** DONE pass a translated representation of the Query in the Job.
      CLOSED: [2012-03-12 Mon 19:57]
      CLOCK: [2012-03-12 Mon 15:30]--[2012-03-12 Mon 17:30] =>  2:00
      CLOCK: [2012-03-12 Mon 09:00]--[2012-03-12 Mon 14:00] =>  5:00
* The Language
* Parsing and Translation
** Translation
The process is:
- setup the Query Input
- setup Window Functions
- setup Table Functions
- setup Map Phase
- if Query has a table function: setup Input Partition and Order
  columns
- setup Output
- setup Where Clause
*** Setup the Query Input
- If the Input to the Query is a _hive Query_ execute it, store result in a Temp. table and change Query 
  * input to the Temp. table.
- extract SerDe, OI information from the Hive metadata for the input
  table.
- If the Query has no table function:
  - validate columns in PArtionSpec and OrderSpec
  - setup ColumnDef and OrderDef for them. These defs point to the
    Fields in the OI.
*** Setup Window Functions
- conversion of a FuncSpec to IWnFn done by FunctionTranslator
  - check function name
  - get function annotation
  - check if Function supports window: throw error if wdw specified
    and fn doesn't support window
  - parse the Groovy expressions for the Boundaries in the Window
  - to the extent possible check that the Boundaries make sense: start
    <= end.
  - create the WnFn instance; set the Window and Query Input Order Columns.
  - check and setup args for WFn:
    - for Groovy Scripts: parse script and set script as attr on Fn Obj.
    - for Groovy Expressions: parse and run scripts and set output as attr on Fn Obj.
    - for other types: evaluate expr and set output as attr on Fn Obj.
*** Setup Table Functions
- The /QuerySpec/ has a list of /TblFuncSpec/ which is in the order
  encountered in the Query. The functions  need to be applied in
  reverse order.
- If windowing clauses are specified in the Query then the final
  function will be the /WindowingTableFunction/.
- translation of a Table function:
  - validate name
  - check if a Window Spec is valid on a TFn.
  - setup a Window Defn and validate Window, see /Setup Wdw Fns/
  - instantiate the TableFunc and set the Window on it.
  - check and setup args: just like in a Wdw Function
  - call /completeTranslation/ on the TableFn passing the Shell, Query
    and TblFunSpec. Used by function to learn about its Input Shape;
    setup Output shape.
- setup TableFunc chain that is in /execution order/
- If Query has Window clauses setup a WindowingTableFunction.
*** Setup Map Phase
If the query's inputtableFunction has a Map Phase then the MR Job has
to be setup such that:
1. During the Map Phase each Map Task collects the Input rows into a
   Partition. At the end it calls the TableFunction's mapExecute
   method.
2. The Map Task takes the output of the mapExecute method and serializes it using the TblFunc's SerDe.
3. The Reduce phase works as always, except that it uses the TblFunc's SerDe to deserialize objects from the stream.

In order for the above to work the QueryMapPhase captures information
on the shape & serilization of data as it gets operated on in the Map
Phase:
1. qry.mapPhase.inputOI is set to the Query's input ObjectInspector
2. qry.mapPhase.inputDeserializer is set to the Query's input Deserializer.
3. qry.mapPhase.outputSerDe is set to the TableFunction's MapOutputSerDe
4. finally the Query's inputOI & deserializer are set to the mapPhase's
outputOI and outputSerDe. The variables inputOI and deserializer are
really used during the reduce phase, since there was no map phase
before there names imply that they represent the Query input.
*** Resetup Partition and Order defns for Query if Fn has map phase.

*** Setup Output
1. Setup Output Columns
   - Validate Column: Column must refer to an input column or a column in
   the top TblFunc output.
   - For selcet columns with expressions: parse the Groovy expressions.
   - Infer the Type of the Column:
     - if explicitly specified by user, use this.
     - if name matches an input column, use type of input column
     - if name matches a column from the query's final table function, use its type.
     - otherwise assume type is 'double'.
2. Validate Output Spec:
   - validate output Serde. By default use: TypedBytesSerDe
   - validate output rowformat. y default use TypedBytesRecordWriter
   - In MR mode output path must be specified.
   - In MR mode if table is specified: verify its existence.
   - In MR mode use SerDe and Properties from output table.
   - In MR mode use outputformat from destination table.
   - By default use TextOutputFormat for outputFormat
3. Setup the Output SerDe & OI
   - build ColumnName and Type list from Select List
   - create a SerDe instance
   - set Col Names and Types in a Propeties obj
   - invoke initialize on SerDe.
   - ask SerDe for an OI
   - setup a ProcessingOI which assumes Java objects as representation.
4. Setup a Writer
   - in MR do nothing
   - instantiate a Writer
   - initialize it, passing it a OutputStream
*** Setup Where Clause
- Parse Where Expression.
** Query componentization
- refers to the process of converting a Query into a set of component
  Queries
- The Table Function Chain is split at the following positions:
  - First function always tied to Query Component 1.
  - Any other TableFunc triggers a Function Chain split if it has to
    Partition its input.

In order to understand the splitting logic one must be aware of the
semantics of the language:
- A tableSpec is a hiveQuery or HiveTbl or HdfsLoc or a TableFunc + a
  Partitioning Specification.
- A TblFunc operates on a TblSpec
- The Query's input is a TblSpec.
  - but if the Query's input is a Function Chain then the topmost
    TFunc must not have a associated Partition Spec.
- internally a QuerySpec is setup by associating the Partition Spec
  with the TFunction that operates on an TblSpec and not with the
  TblSpec.
  - in case there is no TFunc then a NoopTFunc or WindowingTFunc is assumed.

The splitting is done in the following way:
- The current QuerySpec is cloned into 2 QSpecs: current, rest.
- For the 'current' QSpec:
  - The function chain is set from the input function up to the
    function previous to the split position.
  - The whereExpr is cleared.
  - windowing clauses are cleared
  - The selectList is set to Columns based on the previous function's OutputShape.
  - The QuerySpec's tableOutput is changed so that the outputPath is a
    jobWorking directory; the output SerDe and properties are
    LazyBinarySerDe. A Temporary table is created based on the
    OutputShape of the previous function. The QuerySpec's output
    tableName is set to this Table.
- For the 'rest' QSpec:
  - The function chain is set from the split position upto to the end.
  - The QuerySpec's tableInput is changed to point the table created
    above. Hive Query is set to a non null String to trigger deletion
    of the table at the end of this job. QuerySpec's inputSerDe,
    format is set based on ColumnarSerDe. 
  - set Query's partition & order clause based on function's partition
    & order spec. 
- The split algorithm is recursively applied on the 'rest' QuerySpec.
* A Windowing Function
* Execution
** Object conversions
*Non MR Executor*
- qryIn.deserializer converts to Row
- ObjectInspectorUtils.copyToStandardObject converts to Java Object
- passed to Partition

* Where Clause handling
- currently where clause is parsed but not translated
  - where clause is a Groovy expression
- set it on Query; compile it
- in Executor apply where expression to each row and output only rows that eval to true

* Hive Client
- WindowingShell has a HiveClient that can execute a Query
- 2 implementations of HiveClient:
  - for tests: connects to HiveServer
  - in WindowingServer asks Client to execute Query

* Functions
** NPath

*Signature:* NPath(String Pattern, GroovyExpr Symbols, GroovyExpr
 Results)

Returns rows that meet a specified pattern. Use /Symbols/ to specify a
list of expressions to match. /Pattern/ is used to specify a Path.
The results list can contain expressions based on the input columns
and also the matched Path.

- Pattern :: pattern for the Path. Path is 'dot' separated list of
             symbols. Each element is treated as a groovy
             expression. Elements that end in '*' or '+' are
             interpreted with the usual meaning of zero or more, one
             or more respectively. For e.g. "LATE.EARLY*.(ONTIME ||
             EARLY)"  implies a sequence of flights where the first
             occurence was LATE, followed by zero or more EARLY
             flights, followed by a ONTIME or EARLY flight.
- Symbols :: specify a map from names to expresssions. For
             e.g. <[LATE: "arrival\_delay > 0", EARLY: "arrival\_delay <
             0" , ONTIME : "arrival\_delay == 0"]> The
             names for symbols don't need to be quoted as long as they
             are valid groovy names. When in doubt add quotes, for eg:
             <LATE: "arrival\_delay > 0 and arrival\_delay < 60",
             "LATE*2": "arrival\_delay >= 60" >
- Results :: specified as a list. Each entry can be just a string, or
             a  list of 3 elems: [expr, type, name]. If an element is
             just a string, it is interpreted as a reference to a
             column in the input to this function or as a Symbol. When
             specified as a list the first element is interepreted as
             a groovy expression; the second is interpreted as a
             typename, and the third is the expression's alias. For eg
             <["weight", ["2*weight", "double", 'doubleWeight"]>. The expressions are evaluated in the
             context where all the input columns are available, plus
             the attributes "path", "count", "first", and "last" are
             available. Path is a collection of nodes that represents
             the matching Path, count, first, last are convenience fns
             about the Path. Each node in the path exposes all the
             attributes of the corresponding input row.
*** Examples
**** Late Flights
list incidents where a Flight(to NY) has been more than 15 minutes late 5 or more times in a row.
#+begin_src sql
	  from npath(<select origin_city_name, year, month, day_of_month, arr_delay, fl_num
			from flightsdata
			where dest_city_name = 'New York' and dep_time != ''>
	  		partition by fl_num
	  		order by year, month, day_of_month,
	  	'LATE.LATE.LATE.LATE.LATE+',
		<[LATE : "arr_delay \\> 15"]>,
		<["origin_city_name", "fl_num", "year", "month", "day_of_month",
				["(path.sum() { it.arr_delay})/((double)count)", "double", "avgDelay"],
				["count", "int", "numOfDelays"]
		]>)
	  select origin_city_name, fl_num, year, month, day_of_month, avgDelay, numOfDelays
	  into path='/tmp/wout'
	  serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
	  with serdeproperties('field.delim'=',')
	  format 'org.apache.hadoop.mapred.TextOutputFormat'
#+end_src sql
** Other Oracle Functions
*** Frequent Itemsets
mechanism for counting how often multiple events occur together. for
e.g. how often someone has purchased milk & cereal. Input to itemsets
is a collection of items, an itemset.(like the products a given
customer has purchaed)
#+begin_src sql
dbms_frequent_itemset.fi_transactional:
input data is in row format(each transaction is spread across
multip,le rows). Supports: threshol, minimum itemset length, max
length, inclusion set and exclusion items set.
#+end_src sql
*** Width_Bucket function
dual of NTile: output equal width histogram as opposed equal height
histogram:
 : width_bucket(expr, lowValue, highValue, num_of_buckets)
*** case Expressions
- historgram use case
*** Data densification for Reporting
- using partition right join
- to a table function, provide a set of dimension lists
- output original facts plus any missing facts.
- for e.g. output sales for all weeks, even there are no sales for
  some products in certain weeks
* Testing/Use cases
** Setup of Census tables
  
** Eg from Benjamin Poserow
#+begin_src sql
from < select symbol, dt, cast(close AS FLOAT) as close 
       from raw_symbols
     >
partition by symbol
order by dt
with 
    avg(<close>) over rows between unbounded preceding an current row as rollingavg
select symbol, dt, rollingavg
#+end_src sql
** RITA data: airline on-time data
Research and Innovative Technology Administration, Bureau of
Transportation Statistics
http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time

* Advanced Analytical Capabilities
** Why
** How
*** MDX
