-m hive -q "from tableinput(
			 recordreaderclass='com.sap.hadoop.windowing.io.TableWindowingInput',
       keyClass='org.apache.hadoop.io.Text', 
       valueClass='org.apache.hadoop.io.Text',
			 inputPath='c:/Temp/partsmall',
			 inputformatClass='org.apache.hadoop.mapred.TextInputFormat',
			 serdeClass='org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',
			 columns = 'p_partkey,p_name,p_mfgr,p_brand,p_type,p_size,p_container,p_retailprice,p_comment',
			 'columns.types' = 'int,string,string,string,string,int,string,double,string'
			  )
partition by p_mfgr
order by p_mfgr, p_name
with
  rank() as r,
  sum(p_size) over rows between unbounded preceding and current row as s,
  sum(p_size) over rows between current row and unbounded following as s1,
	min(<p_size>) over rows between 2 preceding and 2 following as m[int],
  denserank() as dr,
  cumedist() as cud,
  percentrank() as pr,
  ntile(<3>) as nt,
  count(<p_size>) as c,
  count(<p_size>, 'all') as ca,
  count(<p_size>, 'distinct') as cd,
  avg(<p_size>) as avg, stddev(p_size) as st,
	first_value(p_size) as fv, last_value(p_size) as lv,
  first_value(p_size, 'true') over rows between 2 preceding and 2 following as fv2
select p_mfgr,p_name, p_size, r, s, s1, m, dr, cud, pr, nt, c, ca, cd, avg, st, fv,lv, fv2
where <r \\> 0>"
-i c:/Temp/part.bytes